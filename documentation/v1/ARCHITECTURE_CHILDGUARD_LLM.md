# üõ°Ô∏è ChildGuard-LLM - Architecture Documentation

## üìë Table des Mati√®res

- [WHY - Pourquoi ce syst√®me](#why---pourquoi-ce-syst√®me)
  - [Vision & Mission](#vision--mission)
  - [Probl√©matiques r√©solues](#probl√©matiques-r√©solues)
  - [Valeur ajout√©e](#valeur-ajout√©e)
  - [Contexte m√©tier](#contexte-m√©tier)
- [HOW - Architecture et fonctionnement](#how---architecture-et-fonctionnement)
  - [Architecture g√©n√©rale](#architecture-g√©n√©rale)
  - [Patterns architecturaux](#patterns-architecturaux)
  - [Flow de traitement principal](#flow-de-traitement-principal)
  - [Int√©grations externes](#int√©grations-externes)
- [WHAT - Description technique d√©taill√©e](#what---description-technique-d√©taill√©e)
  - [Structure par modules](#structure-par-modules)
  - [API et interfaces](#api-et-interfaces)
  - [Configuration et assets](#configuration-et-assets)
  - [Points d'int√©gration](#points-dint√©gration)

---

# WHY - Pourquoi ce syst√®me

## üéØ Vision & Mission

Welcome to SRL4Children, which stands for **Safety Readiness Level for Children**. Think of it like the "Technology Readiness Level" (TRL) scale used by scientists and engineers, but designed specifically to keep our kids safe in the digital world.

### Why This Matters to Everyone

Artificial Intelligence (AI) and tools like ChatGPT are changing the world. Children are using them for homework, to explore their creativity, and to learn new things. This is amazing! But just like any powerful tool, we need to make sure it's safe.

Imagine a child asking an AI for help with a school project. What if the AI provides information that is inaccurate, too complex for their age, or worse, harmful or inappropriate? This is the problem we are dedicated to solving. We believe that every child deserves to interact with AI that is not only helpful but also safe, supportive, and designed with their well-being in mind.

### Our Vision: A Safer Digital Playground

Our vision is to create a digital world where parents, educators, and children can trust the AI they use. We want to build the "gold standard" for AI child safety.

We envision a future where AI products have a clear **Safety Readiness Level (SRL)** score. This score would act like a nutrition label on food or a rating on a movie, giving you a simple and reliable way to know if an AI tool is appropriate for a specific age group. This empowers you to make informed decisions for your children or students.

### Our Mission: Building the Tools for Trust

Our mission is to build an open, transparent, and reliable framework to measure and benchmark the safety of AI for children. We are creating a system that acts like a team of expert "AI judges." These judges carefully evaluate content based on a wide range of criteria, including:

*   **Safety:** Is the content free from violence, hate speech, or other harmful topics?
*   **Age-Appropriateness:** Is the language and are the concepts suitable for a 7-year-old versus a 14-year-old?
*   **Relevance & Accuracy:** Is the information correct and on-topic?
*   **Ethics:** Does the AI encourage positive values and healthy social norms?

By using a diverse team of AI judges, we ensure our evaluations are fair, consistent, and less prone to individual bias.

### Join Us in Protecting Our Children

This project is more than just code; it's a community effort to build a safer future for the next generation. Whether you are a parent providing feedback, a student helping us test, a researcher improving our methods, or a developer building the next great educational tool, your contribution is vital.

Together, we can ensure that AI becomes a force for good in the lives of children everywhere.

## ‚ö° Probl√©matiques r√©solues

**Identifi√©es √† partir des impl√©mentations existantes :**

1. **√âvaluation multi-crit√®res** (`src/core/criteria_loader.py`, `assets/criteria/`)
   - 14 crit√®res organis√©s en 4 cat√©gories : Safety, Age, Relevance, Ethics
   - Syst√®me modulaire permettant d'ajouter/modifier des crit√®res

2. **Multi-Judge consensus** (`src/core/judge.py` lignes 14-16, `config.yml` lignes 14-16)
   - 2 juges configurables : model_1 ("gpt-oss:20b") et model_2 ("gemma3:27b")
   - Calcul d'agreement score entre juges

3. **Consistency tracking** (`src/core/judge.py`, `config.yml` lignes 18-37)
   - N-passes (3 par d√©faut) avec param√®tres diff√©rents
   - Suivi de variance de coh√©rence
   - D√©tection d'outliers

4. **Pond√©ration multi-niveau** (`src/core/weighting_system.py`, `config.yml` lignes 78-124)
   - Cat√©gories ‚Üí Sous-cat√©gories ‚Üí Crit√®res individuels
   - Calcul de scores agr√©g√©s pond√©r√©s

5. **Support multi-providers** (`src/connectors/clients.py` lignes 7-29)
   - OpenAI, Anthropic, Groq, Mistral, Ollama
   - Interface unifi√©e pour diff√©rents fournisseurs LLM

## üíº Valeur ajout√©e

**Bas√©e sur les fonctionnalit√©s impl√©ment√©es :**

- **Automatisation** : √âvaluation batch de datasets (observable dans `run_benchmark.py`)
- **Fiabilit√©** : Syst√®me multi-juges r√©duisant les biais individuels
- **Tra√ßabilit√©** : Logging d√©taill√© et sauvegarde JSON compl√®te des √©valuations
- **Extensibilit√©** : Syst√®me de crit√®res modulaires dans `assets/criteria/`
- **Reproductibilit√©** : Configuration centralis√©e et versioning des crit√®res

## üè¢ Contexte m√©tier

**Domaine identifi√© √† partir des technologies et patterns :**

- **Domaine** : Intelligence Artificielle - S√©curit√© et √©thique des LLM
- **Secteur** : Protection de l'enfance et contenu g√©n√©r√© par IA
- **Technologies** : Python, LLM APIs, YAML configuration, JSON data exchange
- **Patterns** : Modular architecture, Multi-judge evaluation, Criteria-based assessment

---

# HOW - Architecture et fonctionnement

## üèóÔ∏è Architecture g√©n√©rale

```mermaid
graph TD
    A[run_benchmark.py] --> B[src/core/config.py]
    A --> C[src/data/loader.py]
    A --> D[src/core/judge.py]
    
    D --> E[src/core/criteria_loader.py]
    D --> F[src/core/weighting_system.py]
    D --> G[src/connectors/clients.py]
    
    E --> H[assets/criteria_registry.yml]
    E --> I[assets/criteria/*.prompt]
    
    G --> J[OpenAI API]
    G --> K[Anthropic API]
    G --> L[Groq API]
    G --> M[Mistral API]
    G --> N[Ollama API]
    
    B --> O[config.yml]
    C --> P[data/*.csv]
    
    A --> Q[outputs/]
    Q --> R[JSON records]
    Q --> S[CSV results]
    Q --> T[Benchmark logs]
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
```

**Explication du sch√©ma :**

Ce diagramme repr√©sente l'architecture principale du syst√®me ChildGuard-LLM bas√©e sur l'analyse du code source :

- **Point d'entr√©e** : `run_benchmark.py` (ligne 1-9) orchestre l'ensemble du processus
- **Configuration** : `src/core/config.py` charge `config.yml` pour la configuration centralis√©e
- **Donn√©es** : `src/data/loader.py` charge les datasets depuis `data/*.csv`  
- **C≈ìur d'√©valuation** : `src/core/judge.py` impl√©mente le syst√®me multi-juges V1.1
- **Crit√®res modulaires** : `src/core/criteria_loader.py` charge les crit√®res depuis `assets/criteria/`
- **Pond√©ration** : `src/core/weighting_system.py` calcule les scores agr√©g√©s
- **Connecteurs LLM** : `src/connectors/clients.py` unifie l'acc√®s aux diff√©rents providers

## üîß Patterns architecturaux

**Patterns identifi√©s dans le code :**

1. **Modular Architecture** (`src/` structure)
   - S√©paration claire : core/, connectors/, data/, utils/
   - Chaque module a une responsabilit√© sp√©cifique

2. **Configuration-Driven** (`config.yml`, `assets/criteria_registry.yml`)
   - Param√®tres externalis√©s dans fichiers YAML
   - Assets modulaires dans r√©pertoire d√©di√©

3. **Multi-Judge Pattern** (`src/core/judge.py` lignes 151-189)
   - √âvaluation parall√®le par plusieurs mod√®les
   - Agr√©gation et calcul d'accord entre juges

4. **Strategy Pattern** (`src/connectors/clients.py`)
   - PROVIDERS dict (ligne 47) mapant provider ‚Üí fonction
   - Interface unifi√©e pour diff√©rents backends LLM

5. **Factory Pattern** (`src/core/criteria_loader.py`)
   - Chargement dynamique des crit√®res depuis fichiers
   - Registry centralis√© des crit√®res disponibles

## üîÑ Flow de traitement principal

```mermaid
sequenceDiagram
    participant Main as run_benchmark.py
    participant Config as config.py
    participant Loader as data/loader.py
    participant Judge as core/judge.py
    participant Criteria as criteria_loader.py
    participant LLM as connectors/clients.py
    participant Output as outputs/
    
    Main->>Config: Load config.yml
    Main->>Loader: Load datasets
    Main->>Judge: Initialize SRL4Children
    
    Judge->>Criteria: Load criteria registry
    Criteria-->>Judge: 14 criteria loaded
    
    loop For each prompt/model
        Main->>LLM: Generate response
        LLM-->>Main: LLM response
        
        Main->>Judge: judge_v1_1(prompt, response, age_group)
        
        loop For each criterion (14)
            Judge->>LLM: Evaluate with model_1 (3 passes)
            Judge->>LLM: Evaluate with model_2 (3 passes)
            Judge->>Judge: Calculate agreement score
        end
        
        Judge->>Judge: Aggregate weighted scores
        Judge-->>Main: V1.1 JSON result
        
        Main->>Output: Save detailed JSON
        Main->>Output: Update CSV results
    end
    
    Main->>Output: Generate benchmark logs
```

**Explication d√©taill√©e du flow :**

Ce diagramme de s√©quence est bas√© sur l'analyse du code principal dans `run_benchmark.py` (lignes 355-584) et `src/core/judge.py` (lignes 401-477) :

1. **Initialisation** : Chargement config et donn√©es
2. **Pour chaque test** : G√©n√©ration de r√©ponse par le mod√®le test√©  
3. **√âvaluation multi-juges** : Chaque crit√®re √©valu√© par 2 juges √ó 3 passes
4. **Agr√©gation** : Calcul des scores pond√©r√©s et m√©triques de coh√©rence
5. **Sauvegarde** : JSON d√©taill√© + CSV consolid√© + logs

## üîå Int√©grations externes

**Identifi√©es dans le code source :**

### Providers LLM (`src/connectors/clients.py`)

```python
PROVIDERS = {
    "openai": openai_generate,      # lignes 7-11
    "anthropic": anthropic_generate, # lignes 13-17
    "groq": groq_generate,          # lignes 19-23
    "mistral": mistral_generate,    # lignes 25-29
    "ollama": ollama_generate       # lignes 31-82
}
```

### D√©pendances Python (`requirements.txt`)

- **Core** : pandas, numpy, pyyaml, python-dotenv
- **LLM APIs** : openai, anthropic, requests, ollama  
- **Validation** : jsonschema, pydantic
- **UI/Logging** : colorama, tqdm
- **Dev/Test** : pytest, black, ruff

### APIs externes configur√©es

- **OpenAI** : via OPENAI_API_KEY env var
- **Anthropic** : via ANTHROPIC_API_KEY env var  
- **Groq** : via GROQ_API_KEY env var
- **Mistral** : via MISTRAL_API_KEY env var
- **Ollama** : Configuration host/port dans `config.yml` (lignes 40-62)

---

# WHAT - Description technique d√©taill√©e

## üìÅ Structure par modules

### `/src/core/` - C≈ìur du syst√®me

#### `judge.py` - Syst√®me de jugement V1.1
```python
@dataclass
class JudgeResult:           # lignes 26-35
    judge_id: str
    criterion_id: str
    pass_results: List[Dict[str, Any]]
    final_score: float
    consistency_variance: float
    execution_time_ms: int
    raw_responses: List[str]

class SRL4Children:          # classe principale V1.1
    def evaluate_content(...)  # M√©thode principale d'√©valuation
```

#### `criteria_loader.py` - Chargeur de crit√®res modulaire
```python
@dataclass
class CriterionConfig:       # lignes 16-32
    id: str
    category: str
    subcategory: str
    name: str
    version: str
    description: str
    file: str

class CriteriaLoader:        # lignes 34-162
    def load_registry(...)     # Charge criteria_registry.yml
    def load_criterion(...)    # Charge un crit√®re sp√©cifique
```

#### `weighting_system.py` - Syst√®me de pond√©ration multi-niveau
```python
class WeightingSystem:       # lignes observ√©es dans l'import
    def calculate_weighted_scores(...)  # Calcul scores pond√©r√©s
```

### `/src/connectors/` - Connecteurs LLM

#### `clients.py` - Interface unifi√©e providers
- **openai_generate()** : Client OpenAI (lignes 7-11)
- **anthropic_generate()** : Client Anthropic (lignes 13-17) 
- **groq_generate()** : Client Groq (lignes 19-23)
- **mistral_generate()** : Client Mistral (lignes 25-29)
- **ollama_generate()** : Client Ollama local (lignes 31-82)

### `/src/data/` - Gestion des donn√©es

#### `loader.py` - Chargeur de datasets
```python
def load_all(paths: List[Path]) -> pd.DataFrame  # Inf√©r√© de l'import ligne 17
```

### `/src/utils/` - Utilitaires

#### `colors.py` - Interface color√©e CLI
#### `cli_interface.py` - Interface utilisateur console
#### `config_loader.py` - Helpers configuration

### `/assets/` - Assets modulaires

#### `criteria/` - Crit√®res d'√©valuation (14 crit√®res)
Structure hi√©rarchique observ√©e :
```
criteria/
‚îú‚îÄ‚îÄ safety/
‚îÇ   ‚îú‚îÄ‚îÄ sexual/ (sexual_content__v1_0.prompt, sensual_manipulation__v1_1.prompt)
‚îÇ   ‚îú‚îÄ‚îÄ violence/ (physical_violence__v1_0.prompt, psychological_violence__v1_0.prompt)
‚îÇ   ‚îú‚îÄ‚îÄ manipulation/ (emotional_manipulation__v1_0.prompt)
‚îÇ   ‚îî‚îÄ‚îÄ hate/ (discriminatory_language__v1_0.prompt)
‚îú‚îÄ‚îÄ age/
‚îÇ   ‚îú‚îÄ‚îÄ readability/ (sentence_complexity__v1_0.prompt, vocabulary_level__v1_0.prompt)
‚îÇ   ‚îî‚îÄ‚îÄ cognitive/ (abstract_concepts__v1_0.prompt)  
‚îú‚îÄ‚îÄ relevance/
‚îÇ   ‚îú‚îÄ‚îÄ topic/ (topic_match__v1_0.prompt)
‚îÇ   ‚îî‚îÄ‚îÄ accuracy/ (factual_accuracy__v1_0.prompt)
‚îî‚îÄ‚îÄ ethics/
    ‚îú‚îÄ‚îÄ moral/ (harmful_advice__v1_0.prompt, positive_guidance__v1_0.prompt)
    ‚îî‚îÄ‚îÄ social/ (social_norms__v1_0.prompt)
```

#### `personas.json` - Configurations par groupe d'√¢ge
#### `criteria_registry.yml` - Registry centralis√© des 14 crit√®res

## üîó API et interfaces publiques  

### Interface principale

#### `judge_v1_1()` - Fonction d'√©valuation principale
```python
def judge_v1_1(
    prompt: str,
    response: str, 
    age_group: str,
    config: Dict[str, Any],
    criteria_selection: Optional[str] = None,
    ollama_host: str = "localhost",
    ollama_port: int = 11434
) -> Dict[str, Any]:
```

**Retourne** : JSON V1.1 avec structure (observ√©e dans le code ligne 581-623) :
```json
{
  "final_aggregate_score": float,
  "category_scores": {...},
  "subcategory_scores": {...}, 
  "detailed_criteria": [...],
  "consistency_metrics": {...},
  "metadata": {...}
}
```

### CLI Interface

#### `run_benchmark.py` - Interface ligne de commande
- **Mode interactif** : S√©lection mode attack/defensive, configuration Ollama
- **Configuration Ollama** : 3 presets (local, ssh_tunnel, custom)
- **Post-processing** : Replay des records √©chou√©s
- **Output** : CSV + JSON d√©taill√©s + logs

## ‚öôÔ∏è Configuration et assets

### `config.yml` - Configuration centralis√©e

**Structure observ√©e** :
- `project` : M√©tadonn√©es projet (lignes 6-9)
- `judge_system` : Config multi-juges V1.1 (lignes 12-37) 
- `ollama` : Presets connexion (lignes 40-62)
- `criteria` : Config syst√®me crit√®res (lignes 64-77)
- `weights` : Pond√©ration multi-niveau (lignes 79-124)
- `paths` : Chemins datasets/personas (lignes 127-131)
- `models` : Mod√®les activ√©s (lignes 126-132)

### Assets crit√®res

**Format des fichiers `.prompt`** (exemple `safety/sexual/sexual_content__v1_0.prompt`) :
```yaml
role: "You are an expert child safety evaluator..."
task: "Evaluate if this content contains inappropriate..."  
age_context: "Child is {age_group} years old"
scoring_guide: |
  Score 0: Explicit sexual content...
  Score 5: Completely appropriate...
examples: |
  Example 1: ...
output_format: |
  {{"score": 0-5, "explanation": "...", "evidence_extracts": [...]}}
```

## üîß Points d'int√©gration

### Extensibilit√© des crit√®res
- **Ajout crit√®res** : Cr√©er fichier `.prompt` + mise √† jour `criteria_registry.yml`
- **Versioning** : Support versions dans noms fichiers (ex: `__v1_0`, `__v1_1`)

### Extensibilit√© providers
- **Nouveau provider** : Ajouter fonction dans `clients.py` + mise √† jour `PROVIDERS` dict

### Configuration multi-environnement  
- **Presets Ollama** : local, ssh_tunnel, custom (lignes 47-61 config.yml)
- **Variables d'environnement** : API keys pour providers externes

---

## üìä M√©triques

**TODO : Compl√©ter manuellement les m√©triques existantes.**

---

*Documentation g√©n√©r√©e automatiquement √† partir de l'analyse du codebase ChildGuard-LLM v1.1.0*

[üîó Voir aussi](./API_REFERENCE.md) ‚Ä¢ [üìã D√©ploiement](./DEPLOYMENT_GUIDE.md) ‚Ä¢ [üîß Troubleshooting](./TROUBLESHOOTING.md)
