# ğŸ”§ ChildGuard-LLM - Guide de dÃ©pannage

## ğŸ“‘ Table des MatiÃ¨res

- [Diagnostic gÃ©nÃ©ral](#diagnostic-gÃ©nÃ©ral)
- [Erreurs de configuration](#erreurs-de-configuration)
- [Erreurs de connectivitÃ©](#erreurs-de-connectivitÃ©)
- [Erreurs d'Ã©valuation](#erreurs-dÃ©valuation)
- [ProblÃ¨mes de performance](#problÃ¨mes-de-performance)
- [Logging et monitoring](#logging-et-monitoring)
- [ProblÃ¨mes frÃ©quents](#problÃ¨mes-frÃ©quents)

---

## ğŸ©º Diagnostic gÃ©nÃ©ral

### VÃ©rification de l'environnement

```bash
# VÃ©rifier la version Python
python --version
# Requis : Python â‰¥ 3.8

# VÃ©rifier les dÃ©pendances critiques
python -c "
import pandas, numpy, yaml, json, pathlib
print('âœ… Core dependencies OK')
"

# VÃ©rifier les modules du projet
python -c "
from src.core.judge import judge_v1_1
from src.core.config import get_config
from src.connectors.clients import PROVIDERS
print('âœ… Project modules OK')
"
```

### VÃ©rification de la structure

```bash
# VÃ©rifier les fichiers critiques
test -f config.yml && echo "âœ… config.yml prÃ©sent" || echo "âŒ config.yml manquant"
test -f assets/criteria_registry.yml && echo "âœ… Registry prÃ©sent" || echo "âŒ Registry manquant"
test -d assets/criteria && echo "âœ… CritÃ¨res prÃ©sents" || echo "âŒ CritÃ¨res manquants"
test -f assets/personas.json && echo "âœ… Personas prÃ©sent" || echo "âŒ Personas manquant"

# Compter les critÃ¨res
find assets/criteria -name "*.prompt" | wc -l
# Attendu : 14 critÃ¨res
```

---

## âš™ï¸ Erreurs de configuration

### Erreur : "config.yml not found"

**Source identifiÃ©e** : `src/core/config.py` ligne 72

**Cause** : Fichier de configuration manquant ou mal placÃ©

**Solution** :
```bash
# VÃ©rifier l'emplacement
ls -la config.yml

# Le fichier doit Ãªtre Ã  la racine du projet
# Structure attendue basÃ©e sur l'analyse du code :
pwd  # Doit Ãªtre dans /path/to/SRL4Children/
```

### Erreur : "name 'self' is not defined"

**Source identifiÃ©e** : CorrigÃ© dans le code (ancien bug ligne 600, 605 de `judge.py`)

**Cause** : Utilisation de `self` dans une fonction standalone

**VÃ©rification** :
```bash
# VÃ©rifier que le fix est appliquÃ©
grep -n "judge_system\._aggregate_explanations" src/core/judge.py
grep -n "judge_system\.multi_judge_evaluator\.judges_config" src/core/judge.py
```

### Erreur : "CriteriaLoader.__init__() got an unexpected keyword argument 'base_path'"

**Source identifiÃ©e** : CorrigÃ© dans le code (ancien bug ligne 376 de `run_benchmark.py`)

**Solution vÃ©rifiÃ©e** :
```bash
# VÃ©rifier que le fix est appliquÃ©
grep -n "assets_path=" run_benchmark.py
# Doit contenir : CriteriaLoader(assets_path=...)
```

---

## ğŸ”Œ Erreurs de connectivitÃ©  

### Erreurs APIs externes

#### OpenAI API

**Erreurs typiques** (basÃ©es sur `src/connectors/clients.py` lignes 7-11) :
```
AuthenticationError: Incorrect API key
RateLimitError: Rate limit exceeded
```

**Diagnostic** :
```bash
# VÃ©rifier la variable d'environnement
echo $OPENAI_API_KEY | cut -c1-10  # Affiche les 10 premiers caractÃ¨res

# Test de connexion
python -c "
import os
from src.connectors.clients import openai_generate
try:
    result = openai_generate('Test', model='gpt-4o-mini')
    print('âœ… OpenAI OK')
except Exception as e:
    print(f'âŒ OpenAI Error: {e}')
"
```

#### Ollama

**Erreurs typiques** :
```
ConnectionError: [Errno 111] Connection refused
```

**Diagnostic** :
```bash
# VÃ©rifier le service Ollama
curl -s http://localhost:11434/api/tags || echo "âŒ Ollama non accessible"

# VÃ©rifier les modÃ¨les disponibles
curl -s http://localhost:11434/api/tags | jq '.models[].name' 2>/dev/null

# Test de gÃ©nÃ©ration
python -c "
from src.connectors.clients import ollama_generate
try:
    result = ollama_generate('Test', 'gemma3:4b')
    print('âœ… Ollama OK')
except Exception as e:
    print(f'âŒ Ollama Error: {e}')
"
```

### Configuration rÃ©seau

**SSH Tunnel pour Ollama distant** (basÃ© sur `config.yml` lignes 52-56) :

```bash
# CrÃ©er le tunnel SSH
ssh -L 11435:localhost:11434 user@server-ip -N &
SSH_PID=$!

# VÃ©rifier le tunnel
curl -s http://localhost:11435/api/tags && echo "âœ… Tunnel OK" || echo "âŒ Tunnel KO"

# Nettoyer
kill $SSH_PID
```

---

## ğŸ¤– Erreurs d'Ã©valuation

### Erreur : "Failed to parse judge response"

**Source identifiÃ©e** : ObservÃ©e dans les logs (`judge.py` lignes 335-342)

**Messages typiques** :
```
Failed to parse judge response: Expecting value: line 5 column 5 (char 458)
Failed to parse judge response: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
```

**Causes et solutions** :

1. **JSON malformÃ© du juge LLM**
```python
# Diagnostic dans judge.py
logger.debug(f"Raw response was: {raw_response[:500]}...")
```

2. **ModÃ¨le LLM ne respecte pas le format**
```bash
# VÃ©rifier les prompts de critÃ¨res
head -20 assets/criteria/safety/sexual/sexual_content__v1_0.prompt
# VÃ©rifier la section output_format
```

3. **ProblÃ¨me de tempÃ©rature/top_p**
```yaml
# Dans config.yml lignes 22-30, ajuster :
judge_system:
  evaluation:
    hyperparameters:
      pass_1: 
        temperature: 0.05  # Plus dÃ©terministe
        top_p: 0.8
```

### Erreur : "All criteria evaluations failed"

**Source** : `judge.py` ligne 464

**Diagnostic** :
```bash
# VÃ©rifier les critÃ¨res chargÃ©s
python -c "
from src.core.criteria_loader import CriteriaLoader
loader = CriteriaLoader()
criteria = loader.resolve_criteria_selection('full_evaluation')
print(f'Criteria resolved: {len(criteria)}')
for c in criteria[:5]:  # Premiers 5
    print(f'  - {c}')
"
```

### Scores Ã  zÃ©ro systÃ©matiques

**Diagnostic** (basÃ© sur la logique de `run_benchmark.py` lignes 200-214) :

```python
# Fonction de diagnostic
def is_failed_record(record_data):
    final_score = float(record_data.get("judge_final_score", 0))
    category_scores = record_data.get("judge_category_scores", {})
    return final_score == 0.0 and all(score == 0.0 for score in category_scores.values())
```

**Solutions** :
1. VÃ©rifier les logs d'Ã©valuation
2. Tester avec `test_single.csv` (1 record)
3. RÃ©duire les critÃ¨res : `criteria.default_selection: "basic_safety"`

---

## âš¡ ProblÃ¨mes de performance

### Ã‰valuations lentes

**Temps d'exÃ©cution observÃ©s** (basÃ©s sur les logs) :
- Par critÃ¨re : 90-180 secondes  
- Total pour 14 critÃ¨res : ~30-45 minutes

**Optimisations** :

1. **RÃ©duire les passes** (`config.yml` ligne 20) :
```yaml
judge_system:
  evaluation:
    n_passes: 2  # Au lieu de 3
```

2. **SÃ©lection de critÃ¨res rÃ©duite** :
```yaml
criteria:
  default_selection: "basic_safety"  # Au lieu de "full_evaluation"
```

3. **ModÃ¨les plus rapides** :
```yaml
judge_system:
  judges:
    model_1: "gpt-4o-mini"      # Plus rapide qu'un modÃ¨le 20B
    model_2: "claude-3-haiku"   # Plus rapide que claude-3-sonnet
```

### MÃ©moire insuffisante

**SymptÃ´mes** :
```
MemoryError: Unable to allocate array
OOMKilled: Process was killed due to memory pressure
```

**Solutions** :
```bash
# Traitement par batch plus petits (non implÃ©mentÃ© dans le code actuel)
# Utilisation de datasets plus petits
cp data/test_single.csv data/current_test.csv
```

---

## ğŸ“Š Logging et monitoring

### Configuration des logs

**Niveaux de logging** (basÃ©s sur `run_benchmark.py` lignes 37-43) :

```python
# Modifier le niveau pour plus de dÃ©tails
logging.basicConfig(
    level=logging.DEBUG,  # Au lieu de INFO
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[...]
)
```

### Analyse des logs de benchmark

**Localisation** (basÃ©e sur la structure observÃ©e) :
```bash
# Logs rÃ©cents
ls -lt outputs/*/benchmark_*.log | head -5

# Recherche d'erreurs
grep -i "error\|failed\|exception" outputs/*/benchmark_*.log

# Suivi de progression
tail -f outputs/$(ls -t outputs/ | head -1)/benchmark_*.log
```

### MÃ©triques de cohÃ©rence

**DÃ©tection de problÃ¨mes de cohÃ©rence** (basÃ© sur la structure JSON observÃ©e) :

```python
# Script de diagnostic de cohÃ©rence
import json
from pathlib import Path

def analyze_consistency(json_file):
    with open(json_file) as f:
        data = json.load(f)
    
    result = data['record_data']['judge_v1_1_result']
    metrics = result.get('consistency_metrics', {})
    
    print(f"Overall variance: {metrics.get('overall_variance', 'N/A')}")
    print(f"Judge agreement avg: {metrics.get('judge_agreement_avg', 'N/A')}")
    print(f"Outliers detected: {metrics.get('outliers_detected', 'N/A')}")

# Utilisation
analyze_consistency('outputs/latest/record_1_attack_model.json')
```

---

## ğŸ”„ ProblÃ¨mes frÃ©quents

**TODO : ComplÃ©ter manuellement les problÃ¨mes frÃ©quents vÃ©rifiÃ©s.**

### ProblÃ¨mes de dÃ©marrage

1. **Module non trouvÃ©**
```bash
ModuleNotFoundError: No module named 'src'
# Solution : ExÃ©cuter depuis la racine du projet
cd /path/to/SRL4Children/
python run_benchmark.py
```

2. **Permissions de fichiers**
```bash
PermissionError: [Errno 13] Permission denied: 'outputs/'
# Solution :
chmod -R 755 outputs/
```

### ProblÃ¨mes de donnÃ©es

1. **Dataset format incorrect**
```
KeyError: 'maturity'
# VÃ©rifier le format CSV (voir DEPLOYMENT_GUIDE.md)
```

2. **Personas non trouvÃ©es**
```
FileNotFoundError: assets/personas.json
# VÃ©rifier la structure assets/
```

### ProblÃ¨mes d'output

1. **Espace disque insuffisant**
```bash
# VÃ©rifier l'espace
df -h outputs/
# Nettoyer les anciens rÃ©sultats
find outputs/ -name "*.json" -mtime +7 -delete
```

2. **Fichiers corrompus**
```python
# Validation JSON
import json
try:
    with open('outputs/record.json') as f:
        json.load(f)
    print("âœ… JSON valide")
except json.JSONDecodeError as e:
    print(f"âŒ JSON invalide: {e}")
```

---

## ğŸ” Commandes de diagnostic rapide

### Check complet du systÃ¨me
```bash
#!/bin/bash
echo "=== ChildGuard-LLM Diagnostic ==="

# 1. Environnement
python --version
python -c "import src; print('âœ… Modules OK')" 2>/dev/null || echo "âŒ Modules KO"

# 2. Configuration
test -f config.yml && echo "âœ… Config OK" || echo "âŒ Config manquante"

# 3. Assets
CRITERIA_COUNT=$(find assets/criteria -name "*.prompt" | wc -l)
echo "CritÃ¨res trouvÃ©s: $CRITERIA_COUNT/14"

# 4. Connectivity (si configurÃ©)
curl -s http://localhost:11434/api/tags >/dev/null && echo "âœ… Ollama OK" || echo "â„¹ï¸ Ollama non configurÃ©"

# 5. DerniÃ¨re exÃ©cution
LATEST_LOG=$(ls -t outputs/*/benchmark_*.log 2>/dev/null | head -1)
if [ -n "$LATEST_LOG" ]; then
    echo "Dernier log: $LATEST_LOG"
    grep -c "âœ…" "$LATEST_LOG" && echo "DerniÃ¨re exÃ©cution rÃ©ussie"
fi
```

### RÃ©initialisation complÃ¨te
```bash
# Sauvegarder la configuration
cp config.yml config.yml.backup

# Nettoyer les outputs
rm -rf outputs/*

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt --force-reinstall

# Test minimal
python -c "from src.core.judge import judge_v1_1; print('Reset OK')"
```

---

*Guide de dÃ©pannage basÃ© sur l'analyse du codebase et des logs ChildGuard-LLM v1.1.0*

[ğŸ—ï¸ Architecture](./ARCHITECTURE_CHILDGUARD_LLM.md) â€¢ [ğŸ“– API Reference](./API_REFERENCE.md) â€¢ [ğŸš€ DÃ©ploiement](./DEPLOYMENT_GUIDE.md)